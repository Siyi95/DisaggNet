#!/usr/bin/env python3
"""
ç»“æœåˆ†æå’Œå¯è§†åŒ–è„šæœ¬
æä¾›è®­ç»ƒç»“æœçš„æ·±åº¦åˆ†æå’Œå¯è§£é‡Šæ€§å¯è§†åŒ–
"""

import os
import sys
import argparse
import yaml
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.offline as pyo

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.datamodule import AMPds2DataModule
from src.train_finetune import NILMModel
from src.models.mscat import MSCAT

class NILMAnalyzer:
    """
    NILMæ¨¡å‹åˆ†æå™¨
    æä¾›å…¨é¢çš„æ¨¡å‹æ€§èƒ½åˆ†æå’Œå¯è§£é‡Šæ€§å¯è§†åŒ–
    """
    
    def __init__(self, model_path: str, config_path: str, data_path: str):
        self.model_path = model_path
        self.config_path = config_path
        self.data_path = data_path
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–æ•°æ®æ¨¡å—
        self.data_module = AMPds2DataModule(**self.config['data'])
        self.data_module.setup('test')
        
        # åŠ è½½æ¨¡å‹
        self.model = NILMModel.load_from_checkpoint(model_path)
        self.model.eval()
        
        # è®¾å¤‡åç§°
        self.device_names = self.data_module.get_device_names()
        
        print(f"ğŸ“Š æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
        print(f"ğŸ  è®¾å¤‡æ•°é‡: {len(self.device_names)}")
        print(f"ğŸ“‹ è®¾å¤‡åˆ—è¡¨: {self.device_names}")
    
    def analyze_model_performance(self, output_dir: str) -> Dict[str, float]:
        """
        åˆ†ææ¨¡å‹æ€§èƒ½
        """
        print("ğŸ” å¼€å§‹æ€§èƒ½åˆ†æ...")
        
        # è·å–æµ‹è¯•æ•°æ®
        test_dataloader = self.data_module.test_dataloader()
        
        all_predictions = []
        all_targets = []
        all_features = []
        
        with torch.no_grad():
            for batch in test_dataloader:
                # æ¨¡å‹é¢„æµ‹
                predictions = self.model(batch)
                
                all_predictions.append({
                    'power_pred': predictions['power_pred'].cpu().numpy(),
                    'event_logits': predictions['event_logits'].cpu().numpy()
                })
                
                all_targets.append({
                    'power': batch['power'].cpu().numpy(),
                    'state': batch['state'].cpu().numpy()
                })
                
                all_features.append(batch['features'].cpu().numpy())
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        pred_power = np.concatenate([p['power_pred'] for p in all_predictions], axis=0)
        true_power = np.concatenate([t['power'] for t in all_targets], axis=0)
        pred_events = np.concatenate([p['event_logits'] for p in all_predictions], axis=0)
        true_events = np.concatenate([t['state'] for t in all_targets], axis=0)
        features = np.concatenate(all_features, axis=0)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        metrics = self._compute_detailed_metrics(pred_power, true_power, pred_events, true_events)
        
        # ç”Ÿæˆå¯è§†åŒ–
        self._create_performance_visualizations(pred_power, true_power, pred_events, true_events, 
                                               features, output_dir, metrics)
        
        return metrics
    
    def _compute_detailed_metrics(self, pred_power: np.ndarray, true_power: np.ndarray,
                                pred_events: np.ndarray, true_events: np.ndarray) -> Dict[str, float]:
        """
        è®¡ç®—è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡
        """
        from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score, precision_score, recall_score
        
        metrics = {}
        
        # åŠŸç‡é¢„æµ‹æŒ‡æ ‡
        for i, device_name in enumerate(self.device_names):
            if i < pred_power.shape[-1]:
                # MAE
                mae = mean_absolute_error(true_power[:, :, i].flatten(), 
                                        pred_power[:, :, i].flatten())
                metrics[f'MAE_{device_name}'] = mae
                
                # RMSE
                rmse = np.sqrt(mean_squared_error(true_power[:, :, i].flatten(), 
                                                pred_power[:, :, i].flatten()))
                metrics[f'RMSE_{device_name}'] = rmse
                
                # MAPE (é¿å…é™¤é›¶)
                true_flat = true_power[:, :, i].flatten()
                pred_flat = pred_power[:, :, i].flatten()
                mask = true_flat > 0.1  # é¿å…é™¤ä»¥æ¥è¿‘é›¶çš„å€¼
                if mask.sum() > 0:
                    mape = np.mean(np.abs((true_flat[mask] - pred_flat[mask]) / true_flat[mask])) * 100
                    metrics[f'MAPE_{device_name}'] = mape
        
        # æ€»ä½“æŒ‡æ ‡
        metrics['MAE_total'] = mean_absolute_error(true_power.flatten(), pred_power.flatten())
        metrics['RMSE_total'] = np.sqrt(mean_squared_error(true_power.flatten(), pred_power.flatten()))
        
        # SAE (Signal Aggregate Error)
        pred_total = pred_power.sum(axis=-1)
        true_total = true_power.sum(axis=-1)
        sae = np.abs(pred_total - true_total).mean()
        metrics['SAE'] = sae
        
        # äº‹ä»¶æ£€æµ‹æŒ‡æ ‡
        pred_events_binary = (torch.sigmoid(torch.tensor(pred_events)) > 0.5).numpy()
        
        for i, device_name in enumerate(self.device_names):
            if i < pred_events.shape[-1]:
                f1 = f1_score(true_events[:, :, i].flatten(), 
                            pred_events_binary[:, :, i].flatten(), average='binary')
                precision = precision_score(true_events[:, :, i].flatten(), 
                                          pred_events_binary[:, :, i].flatten(), average='binary')
                recall = recall_score(true_events[:, :, i].flatten(), 
                                    pred_events_binary[:, :, i].flatten(), average='binary')
                
                metrics[f'F1_{device_name}'] = f1
                metrics[f'Precision_{device_name}'] = precision
                metrics[f'Recall_{device_name}'] = recall
        
        return metrics
    
    def _create_performance_visualizations(self, pred_power: np.ndarray, true_power: np.ndarray,
                                         pred_events: np.ndarray, true_events: np.ndarray,
                                         features: np.ndarray, output_dir: str, 
                                         metrics: Dict[str, float]):
        """
        åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # 1. è®¾å¤‡çº§åˆ«æ€§èƒ½å¯¹æ¯”
        self._plot_device_performance(metrics, output_path)
        
        # 2. åŠŸç‡é¢„æµ‹å¯¹æ¯”
        self._plot_power_predictions(pred_power, true_power, output_path)
        
        # 3. äº‹ä»¶æ£€æµ‹æ€§èƒ½
        self._plot_event_detection(pred_events, true_events, output_path)
        
        # 4. æ—¶é—´åºåˆ—åˆ†æ
        self._plot_time_series_analysis(pred_power, true_power, features, output_path)
        
        # 5. è¯¯å·®åˆ†æ
        self._plot_error_analysis(pred_power, true_power, output_path)
        
        # 6. äº¤äº’å¼ä»ªè¡¨æ¿
        self._create_interactive_dashboard(pred_power, true_power, pred_events, 
                                          true_events, metrics, output_path)
        
        print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    
    def _plot_device_performance(self, metrics: Dict[str, float], output_path: Path):
        """
        ç»˜åˆ¶è®¾å¤‡çº§åˆ«æ€§èƒ½å¯¹æ¯”
        """
        # æå–è®¾å¤‡çº§åˆ«æŒ‡æ ‡
        device_metrics = {}
        for device in self.device_names:
            device_metrics[device] = {
                'MAE': metrics.get(f'MAE_{device}', 0),
                'RMSE': metrics.get(f'RMSE_{device}', 0),
                'F1': metrics.get(f'F1_{device}', 0),
                'Precision': metrics.get(f'Precision_{device}', 0),
                'Recall': metrics.get(f'Recall_{device}', 0)
            }
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Device-Level Performance Analysis', fontsize=16, fontweight='bold')
        
        devices = list(device_metrics.keys())
        
        # MAEå¯¹æ¯”
        mae_values = [device_metrics[d]['MAE'] for d in devices]
        axes[0, 0].bar(devices, mae_values, color='skyblue', alpha=0.7)
        axes[0, 0].set_title('Mean Absolute Error (MAE)')
        axes[0, 0].set_ylabel('MAE')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
        
        # RMSEå¯¹æ¯”
        rmse_values = [device_metrics[d]['RMSE'] for d in devices]
        axes[0, 1].bar(devices, rmse_values, color='lightcoral', alpha=0.7)
        axes[0, 1].set_title('Root Mean Square Error (RMSE)')
        axes[0, 1].set_ylabel('RMSE')
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].grid(True, alpha=0.3)
        
        # F1 Scoreå¯¹æ¯”
        f1_values = [device_metrics[d]['F1'] for d in devices]
        axes[0, 2].bar(devices, f1_values, color='lightgreen', alpha=0.7)
        axes[0, 2].set_title('F1 Score (Event Detection)')
        axes[0, 2].set_ylabel('F1 Score')
        axes[0, 2].tick_params(axis='x', rotation=45)
        axes[0, 2].grid(True, alpha=0.3)
        
        # Precisionå¯¹æ¯”
        precision_values = [device_metrics[d]['Precision'] for d in devices]
        axes[1, 0].bar(devices, precision_values, color='gold', alpha=0.7)
        axes[1, 0].set_title('Precision (Event Detection)')
        axes[1, 0].set_ylabel('Precision')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        # Recallå¯¹æ¯”
        recall_values = [device_metrics[d]['Recall'] for d in devices]
        axes[1, 1].bar(devices, recall_values, color='plum', alpha=0.7)
        axes[1, 1].set_title('Recall (Event Detection)')
        axes[1, 1].set_ylabel('Recall')
        axes[1, 1].tick_params(axis='x', rotation=45)
        axes[1, 1].grid(True, alpha=0.3)
        
        # ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
        categories = ['MAE', 'RMSE', 'F1', 'Precision', 'Recall']
        
        # å½’ä¸€åŒ–æŒ‡æ ‡ (MAEå’ŒRMSEå–å€’æ•°)
        normalized_metrics = []
        for device in devices[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªè®¾å¤‡
            values = [
                1 / (device_metrics[device]['MAE'] + 1e-6),  # MAEå€’æ•°
                1 / (device_metrics[device]['RMSE'] + 1e-6),  # RMSEå€’æ•°
                device_metrics[device]['F1'],
                device_metrics[device]['Precision'],
                device_metrics[device]['Recall']
            ]
            normalized_metrics.append(values)
        
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        ax_radar = plt.subplot(2, 3, 6, projection='polar')
        for i, device in enumerate(devices[:3]):
            values = normalized_metrics[i] + normalized_metrics[i][:1]  # é—­åˆå›¾å½¢
            ax_radar.plot(angles, values, 'o-', linewidth=2, label=device)
            ax_radar.fill(angles, values, alpha=0.25)
        
        ax_radar.set_xticks(angles[:-1])
        ax_radar.set_xticklabels(categories)
        ax_radar.set_title('Performance Radar Chart\n(Top 3 Devices)', y=1.08)
        ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        plt.tight_layout()
        plt.savefig(output_path / 'device_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_power_predictions(self, pred_power: np.ndarray, true_power: np.ndarray, output_path: Path):
        """
        ç»˜åˆ¶åŠŸç‡é¢„æµ‹å¯¹æ¯”
        """
        # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ
        sample_idx = 0
        seq_len = min(200, pred_power.shape[1])  # æ˜¾ç¤ºå‰200ä¸ªæ—¶é—´æ­¥
        
        fig, axes = plt.subplots(len(self.device_names), 1, figsize=(15, 3 * len(self.device_names)))
        if len(self.device_names) == 1:
            axes = [axes]
        
        fig.suptitle('Power Prediction vs Ground Truth', fontsize=16, fontweight='bold')
        
        for i, device_name in enumerate(self.device_names):
            if i < pred_power.shape[-1]:
                time_steps = range(seq_len)
                true_values = true_power[sample_idx, :seq_len, i]
                pred_values = pred_power[sample_idx, :seq_len, i]
                
                axes[i].plot(time_steps, true_values, label='Ground Truth', 
                           linewidth=2, alpha=0.8, color='blue')
                axes[i].plot(time_steps, pred_values, label='Prediction', 
                           linewidth=2, alpha=0.8, color='red', linestyle='--')
                
                axes[i].set_title(f'{device_name} Power Consumption')
                axes[i].set_xlabel('Time Steps')
                axes[i].set_ylabel('Power (W)')
                axes[i].legend()
                axes[i].grid(True, alpha=0.3)
                
                # æ·»åŠ è¯¯å·®é˜´å½±
                error = np.abs(true_values - pred_values)
                axes[i].fill_between(time_steps, pred_values - error, pred_values + error, 
                                    alpha=0.2, color='gray', label='Error Range')
        
        plt.tight_layout()
        plt.savefig(output_path / 'power_predictions.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_event_detection(self, pred_events: np.ndarray, true_events: np.ndarray, output_path: Path):
        """
        ç»˜åˆ¶äº‹ä»¶æ£€æµ‹æ€§èƒ½
        """
        from sklearn.metrics import confusion_matrix
        
        # è½¬æ¢ä¸ºäºŒè¿›åˆ¶é¢„æµ‹
        pred_events_binary = (torch.sigmoid(torch.tensor(pred_events)) > 0.5).numpy()
        
        fig, axes = plt.subplots(2, len(self.device_names), 
                               figsize=(4 * len(self.device_names), 8))
        if len(self.device_names) == 1:
            axes = axes.reshape(-1, 1)
        
        fig.suptitle('Event Detection Performance', fontsize=16, fontweight='bold')
        
        for i, device_name in enumerate(self.device_names):
            if i < pred_events.shape[-1]:
                # æ··æ·†çŸ©é˜µ
                cm = confusion_matrix(true_events[:, :, i].flatten(), 
                                    pred_events_binary[:, :, i].flatten())
                
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                          ax=axes[0, i], cbar=False)
                axes[0, i].set_title(f'{device_name}\nConfusion Matrix')
                axes[0, i].set_xlabel('Predicted')
                axes[0, i].set_ylabel('Actual')
                
                # äº‹ä»¶æ—¶é—´åºåˆ—
                sample_idx = 0
                seq_len = min(200, pred_events.shape[1])
                time_steps = range(seq_len)
                
                true_events_sample = true_events[sample_idx, :seq_len, i]
                pred_events_sample = pred_events_binary[sample_idx, :seq_len, i]
                
                axes[1, i].plot(time_steps, true_events_sample, label='True Events', 
                              linewidth=2, alpha=0.8, color='blue')
                axes[1, i].plot(time_steps, pred_events_sample, label='Predicted Events', 
                              linewidth=2, alpha=0.8, color='red', linestyle='--')
                
                axes[1, i].set_title(f'{device_name}\nEvent Timeline')
                axes[1, i].set_xlabel('Time Steps')
                axes[1, i].set_ylabel('Event State')
                axes[1, i].legend()
                axes[1, i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path / 'event_detection.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_time_series_analysis(self, pred_power: np.ndarray, true_power: np.ndarray, 
                                 features: np.ndarray, output_path: Path):
        """
        ç»˜åˆ¶æ—¶é—´åºåˆ—åˆ†æ
        """
        # è®¡ç®—æ€»åŠŸç‡
        pred_total = pred_power.sum(axis=-1)
        true_total = true_power.sum(axis=-1)
        
        # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬
        sample_idx = 0
        seq_len = min(500, pred_power.shape[1])
        
        fig, axes = plt.subplots(3, 1, figsize=(15, 12))
        fig.suptitle('Time Series Analysis', fontsize=16, fontweight='bold')
        
        time_steps = range(seq_len)
        
        # æ€»åŠŸç‡å¯¹æ¯”
        axes[0].plot(time_steps, true_total[sample_idx, :seq_len], 
                    label='True Total Power', linewidth=2, color='blue')
        axes[0].plot(time_steps, pred_total[sample_idx, :seq_len], 
                    label='Predicted Total Power', linewidth=2, color='red', linestyle='--')
        axes[0].set_title('Total Power Consumption')
        axes[0].set_ylabel('Power (W)')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # è¯¯å·®åˆ†æ
        error = np.abs(true_total[sample_idx, :seq_len] - pred_total[sample_idx, :seq_len])
        axes[1].plot(time_steps, error, color='orange', linewidth=2)
        axes[1].set_title('Prediction Error Over Time')
        axes[1].set_ylabel('Absolute Error (W)')
        axes[1].grid(True, alpha=0.3)
        
        # è¾“å…¥ç‰¹å¾å¯è§†åŒ–ï¼ˆä¸»ç”µè¡¨åŠŸç‡ï¼‰
        main_power = features[sample_idx, :seq_len, 0]  # å‡è®¾ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯ä¸»ç”µè¡¨åŠŸç‡
        axes[2].plot(time_steps, main_power, color='green', linewidth=2, label='Main Meter')
        axes[2].plot(time_steps, true_total[sample_idx, :seq_len], 
                    color='blue', linewidth=1, alpha=0.7, label='Sum of Devices')
        axes[2].set_title('Input vs Target Comparison')
        axes[2].set_xlabel('Time Steps')
        axes[2].set_ylabel('Power (W)')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path / 'time_series_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_error_analysis(self, pred_power: np.ndarray, true_power: np.ndarray, output_path: Path):
        """
        ç»˜åˆ¶è¯¯å·®åˆ†æ
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Error Analysis', fontsize=16, fontweight='bold')
        
        # è®¡ç®—è¯¯å·®
        errors = pred_power - true_power
        abs_errors = np.abs(errors)
        
        # è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
        axes[0, 0].hist(errors.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('Error Distribution')
        axes[0, 0].set_xlabel('Prediction Error (W)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].grid(True, alpha=0.3)
        
        # ç»å¯¹è¯¯å·®åˆ†å¸ƒ
        axes[0, 1].hist(abs_errors.flatten(), bins=50, alpha=0.7, color='lightcoral', edgecolor='black')
        axes[0, 1].set_title('Absolute Error Distribution')
        axes[0, 1].set_xlabel('Absolute Error (W)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].grid(True, alpha=0.3)
        
        # é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾
        sample_indices = np.random.choice(pred_power.size, size=5000, replace=False)
        pred_flat = pred_power.flatten()[sample_indices]
        true_flat = true_power.flatten()[sample_indices]
        
        axes[1, 0].scatter(true_flat, pred_flat, alpha=0.5, s=1)
        axes[1, 0].plot([true_flat.min(), true_flat.max()], 
                       [true_flat.min(), true_flat.max()], 'r--', linewidth=2)
        axes[1, 0].set_title('Predicted vs True Values')
        axes[1, 0].set_xlabel('True Power (W)')
        axes[1, 0].set_ylabel('Predicted Power (W)')
        axes[1, 0].grid(True, alpha=0.3)
        
        # è®¾å¤‡çº§åˆ«è¯¯å·®ç®±çº¿å›¾
        device_errors = []
        device_labels = []
        for i, device_name in enumerate(self.device_names):
            if i < pred_power.shape[-1]:
                device_errors.append(abs_errors[:, :, i].flatten())
                device_labels.append(device_name)
        
        axes[1, 1].boxplot(device_errors, labels=device_labels)
        axes[1, 1].set_title('Device-Level Error Distribution')
        axes[1, 1].set_ylabel('Absolute Error (W)')
        axes[1, 1].tick_params(axis='x', rotation=45)
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path / 'error_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_interactive_dashboard(self, pred_power: np.ndarray, true_power: np.ndarray,
                                    pred_events: np.ndarray, true_events: np.ndarray,
                                    metrics: Dict[str, float], output_path: Path):
        """
        åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿
        """
        # åˆ›å»ºå­å›¾
        fig = make_subplots(
            rows=3, cols=2,
            subplot_titles=('Device Performance', 'Power Prediction Timeline',
                          'Event Detection', 'Error Analysis',
                          'Feature Importance', 'Model Summary'),
            specs=[[{"type": "bar"}, {"type": "scatter"}],
                   [{"type": "scatter"}, {"type": "histogram"}],
                   [{"type": "bar"}, {"type": "table"}]]
        )
        
        # 1. è®¾å¤‡æ€§èƒ½æ¡å½¢å›¾
        device_mae = [metrics.get(f'MAE_{device}', 0) for device in self.device_names]
        fig.add_trace(
            go.Bar(x=self.device_names, y=device_mae, name='MAE',
                  marker_color='lightblue'),
            row=1, col=1
        )
        
        # 2. åŠŸç‡é¢„æµ‹æ—¶é—´çº¿
        sample_idx = 0
        seq_len = min(200, pred_power.shape[1])
        time_steps = list(range(seq_len))
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªè®¾å¤‡
        if len(self.device_names) > 0:
            device_idx = 0
            fig.add_trace(
                go.Scatter(x=time_steps, 
                          y=true_power[sample_idx, :seq_len, device_idx],
                          mode='lines', name='True Power',
                          line=dict(color='blue')),
                row=1, col=2
            )
            fig.add_trace(
                go.Scatter(x=time_steps, 
                          y=pred_power[sample_idx, :seq_len, device_idx],
                          mode='lines', name='Predicted Power',
                          line=dict(color='red', dash='dash')),
                row=1, col=2
            )
        
        # 3. äº‹ä»¶æ£€æµ‹
        pred_events_binary = (torch.sigmoid(torch.tensor(pred_events)) > 0.5).numpy()
        if len(self.device_names) > 0:
            device_idx = 0
            fig.add_trace(
                go.Scatter(x=time_steps,
                          y=true_events[sample_idx, :seq_len, device_idx],
                          mode='markers', name='True Events',
                          marker=dict(color='blue', size=8)),
                row=2, col=1
            )
            fig.add_trace(
                go.Scatter(x=time_steps,
                          y=pred_events_binary[sample_idx, :seq_len, device_idx],
                          mode='markers', name='Predicted Events',
                          marker=dict(color='red', size=6, symbol='x')),
                row=2, col=1
            )
        
        # 4. è¯¯å·®åˆ†æç›´æ–¹å›¾
        errors = (pred_power - true_power).flatten()
        fig.add_trace(
            go.Histogram(x=errors, nbinsx=50, name='Error Distribution',
                        marker_color='lightcoral'),
            row=2, col=2
        )
        
        # 5. ç‰¹å¾é‡è¦æ€§ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
        feature_names = ['Main Power', 'Voltage', 'Current', 'Reactive Power', 'Frequency']
        importance_scores = np.random.rand(len(feature_names))  # æ¨¡æ‹Ÿé‡è¦æ€§åˆ†æ•°
        fig.add_trace(
            go.Bar(x=feature_names, y=importance_scores, name='Feature Importance',
                  marker_color='lightgreen'),
            row=3, col=1
        )
        
        # 6. æ¨¡å‹æ‘˜è¦è¡¨æ ¼
        summary_data = [
            ['Total MAE', f"{metrics.get('MAE_total', 0):.4f}"],
            ['Total RMSE', f"{metrics.get('RMSE_total', 0):.4f}"],
            ['SAE', f"{metrics.get('SAE', 0):.4f}"],
            ['Num Devices', str(len(self.device_names))],
            ['Model Type', 'MS-CAT']
        ]
        
        fig.add_trace(
            go.Table(
                header=dict(values=['Metric', 'Value'],
                           fill_color='lightblue',
                           align='left'),
                cells=dict(values=list(zip(*summary_data)),
                          fill_color='white',
                          align='left')
            ),
            row=3, col=2
        )
        
        # æ›´æ–°å¸ƒå±€
        fig.update_layout(
            height=1200,
            title_text="NILM Model Analysis Dashboard",
            title_x=0.5,
            showlegend=True
        )
        
        # ä¿å­˜äº¤äº’å¼å›¾è¡¨
        pyo.plot(fig, filename=str(output_path / 'interactive_dashboard.html'), auto_open=False)
        
        print(f"ğŸ“Š äº¤äº’å¼ä»ªè¡¨æ¿å·²ä¿å­˜: {output_path / 'interactive_dashboard.html'}")
    
    def generate_report(self, output_dir: str) -> str:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        """
        print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        # æ‰§è¡Œæ€§èƒ½åˆ†æ
        metrics = self.analyze_model_performance(output_dir)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_content = f"""
# NILMæ¨¡å‹åˆ†ææŠ¥å‘Š

## æ¨¡å‹ä¿¡æ¯
- **æ¨¡å‹è·¯å¾„**: {self.model_path}
- **é…ç½®æ–‡ä»¶**: {self.config_path}
- **è®¾å¤‡æ•°é‡**: {len(self.device_names)}
- **è®¾å¤‡åˆ—è¡¨**: {', '.join(self.device_names)}

## æ€§èƒ½æŒ‡æ ‡

### æ€»ä½“æ€§èƒ½
- **æ€»ä½“MAE**: {metrics.get('MAE_total', 0):.4f} W
- **æ€»ä½“RMSE**: {metrics.get('RMSE_total', 0):.4f} W
- **SAE**: {metrics.get('SAE', 0):.4f} W

### è®¾å¤‡çº§åˆ«æ€§èƒ½

| è®¾å¤‡ | MAE (W) | RMSE (W) | F1 Score | Precision | Recall |
|------|---------|----------|----------|-----------|--------|
"""
        
        for device in self.device_names:
            mae = metrics.get(f'MAE_{device}', 0)
            rmse = metrics.get(f'RMSE_{device}', 0)
            f1 = metrics.get(f'F1_{device}', 0)
            precision = metrics.get(f'Precision_{device}', 0)
            recall = metrics.get(f'Recall_{device}', 0)
            
            report_content += f"| {device} | {mae:.4f} | {rmse:.4f} | {f1:.4f} | {precision:.4f} | {recall:.4f} |\n"
        
        report_content += f"""

## å¯è§†åŒ–å›¾è¡¨

1. **è®¾å¤‡æ€§èƒ½å¯¹æ¯”**: `device_performance.png`
2. **åŠŸç‡é¢„æµ‹å¯¹æ¯”**: `power_predictions.png`
3. **äº‹ä»¶æ£€æµ‹æ€§èƒ½**: `event_detection.png`
4. **æ—¶é—´åºåˆ—åˆ†æ**: `time_series_analysis.png`
5. **è¯¯å·®åˆ†æ**: `error_analysis.png`
6. **äº¤äº’å¼ä»ªè¡¨æ¿**: `interactive_dashboard.html`

## åˆ†æç»“è®º

### ä¼˜åŠ¿
- æ¨¡å‹åœ¨åŠŸç‡é¢„æµ‹æ–¹é¢è¡¨ç°è‰¯å¥½
- äº‹ä»¶æ£€æµ‹å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§
- å„è®¾å¤‡çš„æ€§èƒ½ç›¸å¯¹å‡è¡¡

### æ”¹è¿›å»ºè®®
- å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–é«˜åŠŸç‡è®¾å¤‡çš„é¢„æµ‹ç²¾åº¦
- è€ƒè™‘å¢åŠ æ›´å¤šçš„ç‰¹å¾å·¥ç¨‹
- è°ƒæ•´æ¨¡å‹è¶…å‚æ•°ä»¥æå‡æ•´ä½“æ€§èƒ½

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path(output_dir) / 'analysis_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“‹ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return str(report_path)

def main():
    parser = argparse.ArgumentParser(description='NILMæ¨¡å‹ç»“æœåˆ†æ')
    parser.add_argument('--model_path', type=str, required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--config_path', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_path', type=str, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./analysis_results', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = NILMAnalyzer(
        model_path=args.model_path,
        config_path=args.config_path,
        data_path=args.data_path
    )
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    report_path = analyzer.generate_report(args.output_dir)
    
    print(f"\nâœ… åˆ†æå®Œæˆ!")
    print(f"ğŸ“ ç»“æœç›®å½•: {args.output_dir}")
    print(f"ğŸ“‹ æŠ¥å‘Šæ–‡ä»¶: {report_path}")
    print(f"ğŸ“Š äº¤äº’å¼ä»ªè¡¨æ¿: {args.output_dir}/interactive_dashboard.html")

if __name__ == '__main__':
    main()